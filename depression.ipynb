{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcefff25",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-06T04:05:54.289551Z",
     "iopub.status.busy": "2024-11-06T04:05:54.288737Z",
     "iopub.status.idle": "2024-11-06T04:06:00.842805Z",
     "shell.execute_reply": "2024-11-06T04:06:00.841319Z"
    },
    "papermill": {
     "duration": 6.562616,
     "end_time": "2024-11-06T04:06:00.845404",
     "exception": false,
     "start_time": "2024-11-06T04:05:54.282788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "/kaggle/input/playground-series-s4e11/sample_submission.csv\n",
      "/kaggle/input/playground-series-s4e11/train.csv\n",
      "/kaggle/input/playground-series-s4e11/test.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 140700 entries, 0 to 140699\n",
      "Data columns (total 20 columns):\n",
      " #   Column                                 Non-Null Count   Dtype  \n",
      "---  ------                                 --------------   -----  \n",
      " 0   id                                     140700 non-null  int64  \n",
      " 1   Name                                   140700 non-null  object \n",
      " 2   Gender                                 140700 non-null  object \n",
      " 3   Age                                    140700 non-null  float64\n",
      " 4   City                                   140700 non-null  object \n",
      " 5   Working Professional or Student        140700 non-null  object \n",
      " 6   Profession                             104070 non-null  object \n",
      " 7   Academic Pressure                      27897 non-null   float64\n",
      " 8   Work Pressure                          112782 non-null  float64\n",
      " 9   CGPA                                   27898 non-null   float64\n",
      " 10  Study Satisfaction                     27897 non-null   float64\n",
      " 11  Job Satisfaction                       112790 non-null  float64\n",
      " 12  Sleep Duration                         140700 non-null  object \n",
      " 13  Dietary Habits                         140696 non-null  object \n",
      " 14  Degree                                 140698 non-null  object \n",
      " 15  Have you ever had suicidal thoughts ?  140700 non-null  object \n",
      " 16  Work/Study Hours                       140700 non-null  float64\n",
      " 17  Financial Stress                       140696 non-null  float64\n",
      " 18  Family History of Mental Illness       140700 non-null  object \n",
      " 19  Depression                             140700 non-null  int64  \n",
      "dtypes: float64(8), int64(2), object(10)\n",
      "memory usage: 21.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler, PolynomialFeatures, OneHotEncoder\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('/kaggle/input/playground-series-s4e11/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/playground-series-s4e11/test.csv')\n",
    "\n",
    "train_df.head()\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b6e984",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T04:06:00.855823Z",
     "iopub.status.busy": "2024-11-06T04:06:00.854937Z",
     "iopub.status.idle": "2024-11-06T04:06:01.098713Z",
     "shell.execute_reply": "2024-11-06T04:06:01.097624Z"
    },
    "papermill": {
     "duration": 0.251564,
     "end_time": "2024-11-06T04:06:01.101326",
     "exception": false,
     "start_time": "2024-11-06T04:06:00.849762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "numeric_features = train_df.select_dtypes(include=[np.float64, np.int64]).columns.tolist()\n",
    "numeric_features.remove('Depression')\n",
    "numeric_features.remove('id')\n",
    "\n",
    "categorical_features = train_df.select_dtypes(include=[object]).columns.tolist()\n",
    "categorical_features.remove(\"Name\")\n",
    "\n",
    "# Fill missing values\n",
    "train_df[numeric_features] = train_df[numeric_features].fillna(train_df[numeric_features].mean())\n",
    "test_df[numeric_features] = test_df[numeric_features].fillna(test_df[numeric_features].mean())\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = RobustScaler()\n",
    "train_df[numeric_features] = scaler.fit_transform(train_df[numeric_features])\n",
    "test_df[numeric_features] = scaler.transform(test_df[numeric_features])\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(2, interaction_only=True, include_bias=False)\n",
    "train_poly = poly.fit_transform(train_df[numeric_features])\n",
    "test_poly = poly.transform(test_df[numeric_features])\n",
    "poly_features = poly.get_feature_names_out(numeric_features)\n",
    "\n",
    "train_poly_df = pd.DataFrame(train_poly, columns=poly_features)\n",
    "test_poly_df = pd.DataFrame(test_poly, columns=poly_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30401f65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T04:06:01.110183Z",
     "iopub.status.busy": "2024-11-06T04:06:01.109858Z",
     "iopub.status.idle": "2024-11-06T04:06:03.738861Z",
     "shell.execute_reply": "2024-11-06T04:06:03.737844Z"
    },
    "papermill": {
     "duration": 2.635912,
     "end_time": "2024-11-06T04:06:03.741135",
     "exception": false,
     "start_time": "2024-11-06T04:06:01.105223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 93800 entries, 0 to 93799\n",
      "Columns: 383 entries, Age to Family History of Mental Illness_Yes\n",
      "dtypes: float64(383)\n",
      "memory usage: 274.1 MB\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding for Categorical Features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "train_encoded = encoder.fit_transform(train_df[categorical_features])\n",
    "test_encoded = encoder.transform(test_df[categorical_features])\n",
    "encoded_features = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "train_encoded_df = pd.DataFrame(train_encoded, columns=encoded_features)\n",
    "test_encoded_df = pd.DataFrame(test_encoded, columns=encoded_features)\n",
    "\n",
    "# Combine encoded features with polynomial features\n",
    "\n",
    "train_processed = pd.concat([train_poly_df, train_encoded_df], axis=1)\n",
    "test_processed = pd.concat([test_poly_df, test_encoded_df], axis=1)\n",
    "test_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30137c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T04:06:03.750587Z",
     "iopub.status.busy": "2024-11-06T04:06:03.749639Z",
     "iopub.status.idle": "2024-11-06T04:06:14.923362Z",
     "shell.execute_reply": "2024-11-06T04:06:14.922313Z"
    },
    "papermill": {
     "duration": 11.180839,
     "end_time": "2024-11-06T04:06:14.925861",
     "exception": false,
     "start_time": "2024-11-06T04:06:03.745022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Resample the data to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(train_processed, train_df['Depression'])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_resampled)\n",
    "test_X_scaled = scaler.transform(test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a06381f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T04:06:14.935472Z",
     "iopub.status.busy": "2024-11-06T04:06:14.934520Z",
     "iopub.status.idle": "2024-11-06T04:06:15.292856Z",
     "shell.execute_reply": "2024-11-06T04:06:15.291748Z"
    },
    "papermill": {
     "duration": 0.366062,
     "end_time": "2024-11-06T04:06:15.295749",
     "exception": false,
     "start_time": "2024-11-06T04:06:14.929687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_resampled, dtype=torch.float32)\n",
    "test_X_tensor = torch.tensor(test_X_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a26b1ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T04:06:15.307537Z",
     "iopub.status.busy": "2024-11-06T04:06:15.306420Z",
     "iopub.status.idle": "2024-11-06T04:06:15.359237Z",
     "shell.execute_reply": "2024-11-06T04:06:15.358131Z"
    },
    "papermill": {
     "duration": 0.061336,
     "end_time": "2024-11-06T04:06:15.361900",
     "exception": false,
     "start_time": "2024-11-06T04:06:15.300564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212f5b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T04:06:15.373265Z",
     "iopub.status.busy": "2024-11-06T04:06:15.372854Z",
     "iopub.status.idle": "2024-11-06T04:09:59.096686Z",
     "shell.execute_reply": "2024-11-06T04:09:59.095425Z"
    },
    "papermill": {
     "duration": 223.737064,
     "end_time": "2024-11-06T04:09:59.103463",
     "exception": false,
     "start_time": "2024-11-06T04:06:15.366399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.1940, Val Loss: 0.1801, Val Accuracy: 0.9310\n",
      "Epoch 2/50, Train Loss: 0.1746, Val Loss: 0.1695, Val Accuracy: 0.9350\n",
      "Epoch 3/50, Train Loss: 0.1647, Val Loss: 0.1641, Val Accuracy: 0.9373\n",
      "Epoch 4/50, Train Loss: 0.1571, Val Loss: 0.1547, Val Accuracy: 0.9397\n",
      "Epoch 5/50, Train Loss: 0.1510, Val Loss: 0.1528, Val Accuracy: 0.9419\n",
      "Epoch 6/50, Train Loss: 0.1471, Val Loss: 0.1507, Val Accuracy: 0.9412\n",
      "Epoch 7/50, Train Loss: 0.1438, Val Loss: 0.1489, Val Accuracy: 0.9424\n",
      "Epoch 8/50, Train Loss: 0.1405, Val Loss: 0.1463, Val Accuracy: 0.9443\n",
      "Epoch 9/50, Train Loss: 0.1386, Val Loss: 0.1445, Val Accuracy: 0.9455\n",
      "Epoch 10/50, Train Loss: 0.1372, Val Loss: 0.1447, Val Accuracy: 0.9466\n",
      "Epoch 11/50, Train Loss: 0.1342, Val Loss: 0.1424, Val Accuracy: 0.9472\n",
      "Epoch 12/50, Train Loss: 0.1314, Val Loss: 0.1422, Val Accuracy: 0.9470\n",
      "Epoch 13/50, Train Loss: 0.1306, Val Loss: 0.1370, Val Accuracy: 0.9476\n",
      "Epoch 14/50, Train Loss: 0.1299, Val Loss: 0.1368, Val Accuracy: 0.9482\n",
      "Epoch 15/50, Train Loss: 0.1282, Val Loss: 0.1383, Val Accuracy: 0.9480\n",
      "Epoch 16/50, Train Loss: 0.1273, Val Loss: 0.1386, Val Accuracy: 0.9482\n",
      "Epoch 17/50, Train Loss: 0.1263, Val Loss: 0.1350, Val Accuracy: 0.9491\n",
      "Epoch 18/50, Train Loss: 0.1240, Val Loss: 0.1378, Val Accuracy: 0.9489\n",
      "Epoch 19/50, Train Loss: 0.1226, Val Loss: 0.1360, Val Accuracy: 0.9498\n",
      "Epoch 20/50, Train Loss: 0.1233, Val Loss: 0.1396, Val Accuracy: 0.9492\n",
      "Epoch 21/50, Train Loss: 0.1202, Val Loss: 0.1422, Val Accuracy: 0.9490\n",
      "Epoch 22/50, Train Loss: 0.1214, Val Loss: 0.1352, Val Accuracy: 0.9499\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "# Define a simple neural network model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze()\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_tensor.shape[1]\n",
    "model = SimpleNet(input_dim=input_dim).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training settings\n",
    "num_epochs = 50\n",
    "patience = 5  # Early stopping patience\n",
    "\n",
    "# Early stopping variables\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_targets = []\n",
    "    val_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_losses.append(loss.item())\n",
    "            val_targets.extend(targets.cpu().numpy())\n",
    "            val_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "    \n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_preds = (np.array(val_outputs) >= 0.5).astype(int)\n",
    "    val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "          f'Train Loss: {np.mean(train_losses):.4f}, '\n",
    "          f'Val Loss: {val_loss:.4f}, '\n",
    "          f'Val Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print('Early stopping triggered')\n",
    "            break\n",
    "\n",
    "# Load the best model weights\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df579304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-06T04:09:59.115806Z",
     "iopub.status.busy": "2024-11-06T04:09:59.115233Z",
     "iopub.status.idle": "2024-11-06T04:10:00.509142Z",
     "shell.execute_reply": "2024-11-06T04:10:00.507822Z"
    },
    "papermill": {
     "duration": 1.402366,
     "end_time": "2024-11-06T04:10:00.511368",
     "exception": false,
     "start_time": "2024-11-06T04:09:59.109002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created.\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data loader\n",
    "test_dataset = TensorDataset(test_X_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "test_outputs = []\n",
    "with torch.no_grad():\n",
    "    for (inputs,) in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        outputs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        test_outputs.extend(outputs)\n",
    "\n",
    "# Prepare submission\n",
    "test_preds = (np.array(test_outputs) >= 0.5).astype(int)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "# Save submission to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Submission file created.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 10008389,
     "sourceId": 84895,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 251.862276,
   "end_time": "2024-11-06T04:10:03.220984",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-06T04:05:51.358708",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
